<html>
<title>Mike Eder - think/work log</title>

<xmp theme="cyborg" style="display:none;">

###Links:
* GitHub
 * [My Home](http://github.com/mikeder)
* SqweebNet
 * [Home](http://sqweeb.net)
 * [CherryMusic](http://sqweeb.net:8080)
 * [DHT](http://sqweeb.net/dht)

###Todo:
- [ ] Update sqweeb.net/dht
- [ ] Build CherryMusic DB connector/parser script
- [ ] Rewrite player creation script in Python
- [ ] Set up NOC-tools VM (pinger, account-creation, etc.)
- [x] Finish bash script for tools/rsdc (verbose and cronjob)
- [x] Create less priveledged user for cherryM

--------------------------------------------------------------------------------
### Start of Log:
--------------------------------------------------------------------------------
### Sun Dec 28 05:08:53 EST 2014

I am very pleased with how well the music-scraper script is working now. In my
first run it brought down 1.3GB worth of music. I have written a bash script 
that is run every 6hrs via cron job and now the process is fully automated. The
bash script is quite simple, there is a list of subreddits to scrape and a for
loop that runs rsdc.py, copies new files over to the NAS and then cleans the
working directory before the next run. Its a thing of simple beauty.

'''
#!/bin/bash

list=(  acidhouse
        ambientmusic
        astateoftrance
        atmosphericdnb
        bigbeat
        chillmusic
        chillout
        chiptunes
        deephouse
        dnb
        dubstep
        edm
        electro
        electrohouse
        electronicdancemusic
        electronicmagic
        futurefunkairlines
        futuresynth
        house
        liquiddnb
        liquiddubstep
        psybient
        psybreaks
        psytrance
        techno
        trance
        tranceandbass )

cd /home/meder/Downloads/rsdc/out
for i in ${list[@]};
        do
                echo `date`
                echo "### Scraping /r/"$i
                rsdc $i
                echo "### Moving new files --> /media/webrips"
                cp -Ru * /media/webrips/
                echo "Cleaning working directory before next run.."
                rm -rf /home/meder/Downloads/rsdc/out/*
        done
```
There may be a more clean way to copy and clean the files but for now this
works perfectly so I see no need to change it. I may also need to increase the
disk size on the tools VM to allow for more source file storage as I plan on
only cleaning the source/in folder weekly and at this rate the disk may fill up
before the source folder gets cleaned. I need to figure out how long it takes
for the links to drop out of the subreddits getting scraped so that I can
safely clear the source folder and not end up with duplicates. Although since
the copy portion of the script above only updates the target, if a file already
exists on the NAS it won't get copied over again.

Next up is the sqweeb.net/dht update. I'd like to overhaul the site and perhaps
use strapdown.js there too so its just as pretty as this page. Eventually I see
myself replacing the WordPress install with a custom page using
strapdown/bootstrap. For now though, I've got a ton of new music to listen to
:)
--------------------------------------------------------------------------------
### Sat Dec 27 05:34:30 EST 2014

Alright I think I've made enough progress to warrent a new entry. I decided to
use an API wrapper for interfacing with Reddit.
ReddiWrap(https://github.com/derv82/reddiwrap/) does exactly what I need and
isn't as complicated to use as PRAW. Using this also allowed me to clean up
several sections of code:

1. I could remove the duplicate filter on the scrape function since now that
I'm no longer parsing raw html with BeautifulSoup I dont get duplicates. Only 1
url is returned per post.

2. I no longer need to use BeautifulSoup! Its always a good day when you can
have few dependencies.

3. ReddiWrap incorporates limits so that I dont hit the Reddit request limit
randomly when running the script. This will finally allow me to finish my
crontab on the tools VM to scrape multiple subreddits daily.

4. I also cleaned up some unused variables and beautified the indents. Now I
feel comfortable putting a version number on this script (v1.1.0) since I won't
have to make a ton of half ass fixes unless I add new funtionality.

I haven't made this Flask friendly yet and I'm not sure how easy it will be
without breaking everything again. But, now that everything works as I intended
I will start work on making this easier to expand into UI.

#### Update 08:33 EST:

I had to make a slight change to the way the submodule ReddiWrap was handled. I
found that if I did a git clone it broke the import of Web.py. I have made a
changed to ReddiWrap.py and commited/pushed so now RSDC is at v1.1.1

--------------------------------------------------------------------------------
### Sat Dec 27 00:36:37 EST 2014

I didn't make the progress that I wanted to on the API work for RSDC(Reddit:
Scrape, Download, Convert) but I did make some improvements to the way the
current user directory is handled for generating a config and making sure the
working directories are present before downloading and converting files.
Tonight I'm going to try and make some progress on the API implementation,
although at this point I am not 100% sure ill be able to do everything I want
compared to how I handle links now but I think at least grabbing the post title
for use when generating Artist - Track info should be useful.

In addition to the RSDC script updates I'd like to look into the possiblity of
incorporating the whole script into the Flask framework so I can develop a nice
clean UI for those that don't want to use the script in the CLI. This is part
of a long term goal to have all of my utility scripts in a Web App format for
use on the RaspberryPi. Currently my RPi just hosts a simple static page that
holds some links for things on the network but eventually I would like to have
a web UI for several home automation tasks such as auto night lights and HVAC
control and such. Again, I may revisit this post as I make progress through the
night.

--------------------------------------------------------------------------------
### Fri Dec 26 01:15:55 EST 2014

Revisiting my Reddit music scraper script today to implement scraping via the
Reddit API instead of raw html parsing. I will be using the PRAW library to do
the API interaction. https://praw.readthedocs.org/en/v2.1.19/
This should hopefully clear up an issue ive started seeing when running the
script where I get a response 429, which apparently means I'm making requests
to Reddit too frequently. I should be able to pass a subreddit to PRAW have it
pull the hot links and pass them back via a json dictionary which I can then
parse for YouTube links. I will probably return to this entry once I have made
some progress on the new code.

--------------------------------------------------------------------------------
### Mon Dec 22 15:08:09 EST 2014

#### Updating .vimrc

Testing out a new .vimrc to see if it allows for proper formatting when im just
free typing into this log. There goes a nice clean line wrap so I guess its
working as I intend. Nice.

--------------------------------------------------------------------------------
### Mon Dec 22 13:59:44 EST 2014

#### Fixing a possible security hole in CherryM and Updates

   I realized that running a music server that is open to the outside world with a user that has sudo access and write access on my NAS was probably not the 
best idea. Unfortunately I tried to implement my changes while sleep deprived,
 and it resulted in accidentally removing sudo access from my main admin 
account. This wouldn't have been an issue if I had done it after I had 
successfully gotten the cherrymusic server running again with the less 
priveledged user but I hadn't.

   In order to get things going again temporarily I booted back up the old 
stream(Arch) VM, and changed port forwarding again to point back at it instead. This was a nice option that I haven't had to use before but it was nice to have a working backup for a change when I screw something up. I went to sleep at this point and am now writing this the following day having successfully made the 
correct changes to the cherryM(Ubuntu) VM. Now the cherrymusic service runs as a user that doesn't have access to the sudo command and the base directory for 
the music folder that it accesses is no longer writable.

   I have also made a few adjustments to the below procedure for setting up the
server from scratch if I ever need to do it again. In addition I made a snapshot of the cherryM VM so that I can rollback to where it is now with everything
working nicely on a fresh install.

--------------------------------------------------------------------------------
### Thu Dec 18 11:06:35 EST 2014

#### CherryMusic update/migration from Arch Linux to Ubuntu 14.10

After struggling to get the the Arch installation of CherryMusic updated to the latest version I decided it would be easier to migrate back to Ubuntu. 
Apparently I had originally installed CM via the Arch AUR and when the package 
is actually installed versus just a repo clone it makes it more difficult to 
update. Also it seems the 0.34.0 version had a bug that needed to get worked out anyway. So while I figuring out my next steps I waited for the 0.34.1 update to come out before finalizing. At least under Ubuntu/Debian I can run the server as a service like I'm used to doing.
Updating from now on should only require a git pull within the cherrymusic dir.
My steps taken for migration back to Ubuntu were as follows:

- Create new VM on esxi-ts1 host via VMWare vSphere Client
- Install Ubuntu Server 14.10 32bit/x86 on new VM
- Install OpenSSH and Samba(needed for connecting to NAS via cifs)
- Edit /etc/fstab to mount music folder:

```
# Add this line to the bottom of the fstab file (//ts0 is my NAS)
//ts0/Music /mnt/music cifs users,auto,credentials=/etc/.cifsauth 0 0
```

- Edit /etc/.cifsauth to include creds to connect to NAS

```
# Example /etc/.cifsauth - user with read-only access on NAS
username=cherry
password=music
```

- Make the /mnt/music folder and mount the NAS in it

```
$ sudo mkdir /mnt/music
$ sudo mount -a
```

- Install CherryMusic and dependencies
 * Roughly following this blog post: http://fomori.org/blog/?p=687

```
$ sudo apt-get install python3
$ sudo apt-get install mpg123 faad vorbis-tools flac imagemagick lame python3-unidecode
$ sudo apt-get install git
$ sudo adduser cherry
$ su cherry
$ git clone --branch devel git://github.com/devsnd/cherrymusic.git
# Create symlink to music folder for cherry's basedir
$ ln -s /mnt/music/\!SORTED/ /home/cherry/cherrymusic-devel/basedir
# Start server with --setup tag and perfor initial configuration
$ python3 cherrymusic --setup
```

- Connect to localhost:8080 to configure (cherryM:8080 in my case)
- Kill server with ^C so it can be run as a service
- Init script pulled from:
 * https://github.com/Lord-Simon/Scripts/tree/master/cherrymusic
 * Edit /etc/init.d/cherrymusic to fit install

- Once script is install start the server back up as a service

```
$ sudo service cherrymusic start
$ sudo service cherrymusic status
 * CherryMusic 'cherrymusic' is running (pid 5758).
```

- Update DHCP reservations and port forwarding to point to new server.

--------------------------------------------------------------------------------
### Tue Dec 16 21:34:25 EST 2014

For some reason it seems the CherryMusic updates aren't sticking, the about page still shows 0.33.0 despite my local repo being up to date. Further investigation is required.

--------------------------------------------------------------------------------
### Tue Dec 16 08:50:25 EST 2014

This morning I actually finished the CherryMusic upgrade, I got distracted last night by an invite to do the VoG raid in Destiny. Since I had never run the raid before I jumped at the chance, its difficult for me because it requires 6 
players and I dont have that many friends who play. I ended up having to leave 
before we finished anyway. The stage I came in on ended up taking us several hours, probably because I was too low level to do enough damage.

Anyway the CherryMusic upgrade 0.33.0 -> 0.34.0 is done now.
Last night I was trying to do some webrips with my [Reddit music scraper script](https://github.com/sqweebking/music-scraper/blob/master/scrape.py)
and I realized I forgot the command I used for cleaning non-UTF8 characters from
the resulting files so I figured id log my process here.

```
# Scrape a subreddit for new songs
$ rsdc electronicmusic
# Move to the scraped music output directory
$ cd /home/music/scraped/out
# Run detox on all subfolders and files
$ detox -r *
# Copy all new files to the NAS(/mnt/webrips/)
$ cp -Ruv *
```

I had started to write a bash script to do this all for me so that I can just
have a cron job run it for me nightly but I ran into some issues with output
from the scraper script. Id like to retain output so that the cron emails will
give me some insight as to how the scrape is working. Another item for the TODO.

--------------------------------------------------------------------------------
### Mon Dec 15 21:38:50 EST 2014

Updating CherryMusic over @ sqweeb.net to version 0.34.0, here are the steps I used and the changelog for the latest devel version:

```
cd ~/cherrymusic
sudo git pull
sudo systemctl restart cherrymusic@meder.service
```


```
0.34.0 (2014-12-08)

 - FEATURE: sorting of queue and open playlists via dropdown menu
 - FEATURE: option to display active album art in player (thanks to @lzh9102)
 - FEATURE: enable intertial scroll on mobile (thanks to @tkafka)
 - FIXED: updated jPlayer to version 2.7.1 to fix playback on android
 - FIXED: workaround for CherryPy (>=3.3.0) bug when releasing session locks
 - FIXED: authentication for /serve resource (thanks to @susnux for spotting)
 - FIXED: album art search works with new amazon and bestbuy web sites
 - FIXED: udated tinytag to v0.7.2 to decode ID3 tags from latin1
 - FIXED: admin GUI can again create new users as admin
 - FIXED: restore "change password" functionality
 - FIXED: various problems when filecache entries no longer exist on disk
 - FIXED: some quirks in playlist age display were forcefully retired
 - FIXED: server handles bad basedir configuration gracefully
 - FIXED: updated jquery-ui to 1.11.1, does not ruin the js-compression anymore
 - FIXED: localhost_only bind address respects server.ipv6_enabled setting
 - IMPROVEMENT: transcoded tracks have a duration in player (thanks to @lzh9102)
 - IMPROVEMENT: "folder" items are now easier to click
 - IMPROVEMENT: request meta data for one track at a time to reduce server load
 - IMPROVEMENT: expand CI tests to include Python 3.4 and PyPy3
 - IMPROVEMENT: expand CI tests to more versions of CherryPy
 - IMPROVEMENT: easier pypi releases via auto-conversion of README.md -> .rst
```


--------------------------------------------------------------------------------
### Mon Dec 15 17:43:56 EST 2014

Testing out the new .vimrc as seen in:

http://youtu.be/rENXp-AWZmk?list=UUd26IHBHcbtxD7pUdnIgiCw

My .vimrc:
```
colorscheme elflord
syntax on
syntax enable
let @n = '/Log:^Mo^M^M^M^M^[kkkk80i-^[j! date^Mi### ^[^Mjzzi'

```
The last line there is important, it is the macro I can now use to start a new 
entry in the log. When I press @n vim will automatically jump to the bottom of
the 'Start of Log:' line and insert a date/time stamp and put me in insert mode 
ready to type out the entry. Cool stuff, all of which is still very new and 
exciting to me for reasons.

###12/15/14 - Updates
- [x] DNS updated at GoDaddy.com to point mikeder.net to the github.io servers. 
- [x] CNAME file in place in the repo

Thinking through updates and what not, testing TTL. The GoDaddy DNS changes
took a little longer than expected to propagate ~45min. Although this may have
been off due to my browser caching the default 404 page from github.io

Now its just a matter of learing the in's and out's of strapdown and bootstrap
to flesh out this site a bit.

###12/14/14 - Initialized
Tonight I took a few minutes to get a new github account set up to separate 
from my sqweebking/sqweeb.net account and create my first repo for this website.
http://github.com/mikeder -> http://github.com/mikeder/mikeder.github.io
I then cloned this repo locally to create my index.html file. Which for now is
the only hosted file here. I still need to set up the DNS forwarding for the 
GoDaddy domain mikeder.net and put a CNAME file in the repo.

###12/13/14 - Realized
Discovered github.io aka GitHub Pages via this YouTube video: 
http://youtu.be/rENXp-AWZmk?list=UUd26IHBHcbtxD7pUdnIgiCw
I decided to give this a shot for my personal domain mikeder.net, which I
purchased on GoDaddy a couple of months ago. This will be used as a personal
work log and journal for things I may be working on or thinking about it my 
day to day activities.

</xmp>

<script src="/strapdown/v/0.2/strapdown.js"></script>

</html>

